{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the around 100.000 reviews of the most reviewed amazon products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_df = pd.read_csv('../data/amazon_sorted/most_populair_products.csv')\n",
    "amazon_df = amazon_df.drop(columns='index').dropna().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the most reviewed product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8799\n"
     ]
    }
   ],
   "source": [
    "product_df = amazon_df[amazon_df['product_id'] == amazon_df.iloc[0]['product_id']]\n",
    "print(len(product_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a topic model, containing the 10 most important topics of the reviews. \\\n",
    "The topic model ```fit_transform``` function returns predictions, which are two lists: the first is a list of predicted topic per review, the second is a list of topic probability distributions per review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic  Count                           Name\n",
      "0      -1   6544               -1_the_to_and_is\n",
      "1       0    488       0_easy_sound_great_works\n",
      "2       1    250         1_hearing_he_these_and\n",
      "3       2    247               2_tv_watch_my_to\n",
      "4       3    225                3_tv_it_the_can\n",
      "5       4    221             4_the_to_rs120_and\n",
      "6       5    196  5_wireless_the_headphones_and\n",
      "7       6    182         6_the_reception_is_and\n",
      "8       7    154            7_head_they_the_off\n",
      "9       8    151            8_static_the_and_of\n",
      "10      9    141   9_product_good_abc_excellent\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "nr_topics = 10\n",
    "topic_model = BERTopic(nr_topics=nr_topics, calculate_probabilities=True)\n",
    "\n",
    "predictions = topic_model.fit_transform(product_df['review_body'])\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Try topic modeling per sentence instead of per review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the reviews, take the average sentiment value of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is only needed once\n",
    "# import nltk\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "reviews = product_df['review_body'].to_list()\n",
    "compounds = list()\n",
    "for review in reviews:\n",
    "    compounds.append(sia.polarity_scores(review)['compound'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the average sentiment value of each topic, and calculate the deviation of each review from each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sents = [0]*nr_topics\n",
    "cum_weights = [0]*nr_topics\n",
    "topic_model_info = topic_model.get_topic_info().drop([0]).reset_index()\n",
    "for index, compound in enumerate(compounds):\n",
    "    for topic, row in topic_model_info.iterrows():\n",
    "        weight = predictions[1][index][topic]\n",
    "        sents[topic] += weight*compound\n",
    "        cum_weights[topic] += weight\n",
    "average_sents = [0]*nr_topics\n",
    "for i in range(nr_topics):\n",
    "    average_sents[i] = sents[i] / cum_weights[i]\n",
    "\n",
    "## average_sents is now a value between 0 and 2 for each topic\n",
    "\n",
    "deviations = list()\n",
    "for index, review in enumerate(reviews):\n",
    "    deviation_per_topic = list()\n",
    "    for topic in range(nr_topics):\n",
    "        deviation_per_topic.append(np.abs(compounds[index] - average_sents[topic]))\n",
    "    deviations.append(deviation_per_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the $L_2$ norm of each word vector of each review, then calculate the average per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader\n",
    "\n",
    "w2v = gensim.downloader.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_norms = list()\n",
    "for review in reviews:\n",
    "    total_norm = 0\n",
    "    valid_words = 0\n",
    "    for word in review.split():\n",
    "        try:\n",
    "            total_norm += np.linalg.norm(w2v[word])\n",
    "            valid_words += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    try:\n",
    "        avg_norm = total_norm / valid_words\n",
    "        avg_norms.append(avg_norm)\n",
    "    except ZeroDivisionError:\n",
    "        avg_norms.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank all the reviews according to the topic relevance, average sentiment value and information estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = list()\n",
    "alpha = 0.1\n",
    "beta = 0.05\n",
    "topic_relevance = predictions[1]\n",
    "for topic in range(nr_topics):\n",
    "    ranking = pd.DataFrame(data={'review': reviews, 'relevance': np.transpose(topic_relevance)[topic], 'sentiment_deviation': np.transpose(deviations)[topic], 'information': avg_norms})\n",
    "    ranking['score'] = ranking['relevance'] + alpha*ranking['sentiment_deviation'] + beta*ranking['information']\n",
    "    ranking = ranking.sort_values(by=['score'], ascending=False)\n",
    "    ranking.reset_index()\n",
    "    rankings.append(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the reranked 100 reviews of each topic of the most reviewed amazon product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90d3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import truncate\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/textsum-cnn-12-6\", truncation=True)\n",
    "topic_summaries = ''\n",
    "for ranking in rankings:\n",
    "    first_reviews = ranking.head(100)['review']\n",
    "    full_text = ''\n",
    "    for review in first_reviews:\n",
    "        full_text += '\\n' + review\n",
    "    topic_summaries += '\\n' + summarizer(full_text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
    "final_summary = summarizer(topic_summaries, max_length=130, min_length=30, do_sample=False)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers.serialization import df_to_json\n",
    "\n",
    "topsum_path = '../data/topsum_summaries.json'\n",
    "\n",
    "df_to_json(pd.DataFrame(data={'text': [final_summary]}), path=topsum_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the information value of the summary with the next word predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 0, 'total': 50, 'ratio': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.metrics.information_estimator import test_summaries\n",
    "\n",
    "test_summaries(topsum_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the relevance of the summary to the product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.161898877885607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.metrics.relevance_calculator import calculate_relevance\n",
    "\n",
    "calculate_relevance(topsum_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the most common English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2824193.94'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.metrics.word_frequency import count_words\n",
    "\n",
    "count_words(topsum_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('researchTopics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f113d7023f71bfe0e7667ca25b9eab9cdf12911e487b79fffc00c29d69af094b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
