{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the around 100.000 reviews of the most reviewed amazon products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "amazon_df = pd.read_csv('../data/amazon_sorted/most_populair_products.csv')\n",
    "amazon_df = amazon_df.drop(columns='index').dropna().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the most reviewed product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8799\n"
     ]
    }
   ],
   "source": [
    "product_df = amazon_df[amazon_df['product_id'] == amazon_df.iloc[0]['product_id']]\n",
    "print(len(product_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a topic model, containing the 10 most important topics of the reviews. \\\n",
    "The topic model ```fit_transform``` function returns predictions, which are two lists: the first is a list of predicted topic per review, the second is a list of topic probability distributions per review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8799\n",
      "    Topic  Count                           Name\n",
      "0      -1   6038               -1_the_to_and_is\n",
      "1       0    531        0_sound_easy_great_good\n",
      "2       1    328  1_wireless_the_headphones_and\n",
      "3       2    326            2_it_works_this_was\n",
      "4       3    320            3_hearing_he_for_my\n",
      "5       4    294                 4_the_to_tv_it\n",
      "6       5    225            5_the_range_is_good\n",
      "7       6    210             6_the_rs120_to_and\n",
      "8       7    189               7_tv_watch_to_my\n",
      "9       8    189               8_tv_the_and_can\n",
      "10      9    149            9_head_they_off_the\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "nr_topics = 10\n",
    "topic_model = BERTopic(nr_topics=nr_topics, calculate_probabilities=True)\n",
    "\n",
    "predictions = topic_model.fit_transform(product_df['review_body'])\n",
    "print(len(predictions[1]))\n",
    "print(topic_model.get_topic_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([4, 4, 4, 4, 4, 4, 4, 4, 4], array([[4.91686497e-21, 3.36651807e-21, 3.92007461e-21, 3.90391301e-21,\n",
      "        9.99999999e-01, 1.93956031e-21, 1.37972835e-21, 2.46451028e-21,\n",
      "        3.70640189e-21, 5.74249369e-22],\n",
      "       [4.71774979e-02, 3.21554577e-02, 3.75492545e-02, 3.74333981e-02,\n",
      "        3.66036750e-01, 1.85467634e-02, 1.30397106e-02, 2.35903865e-02,\n",
      "        3.55075806e-02, 5.47057395e-03],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.99999999e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [8.75933342e-05, 5.07389297e-05, 1.04429993e-04, 6.39363631e-05,\n",
      "        8.25576562e-01, 2.05937664e-05, 1.00284895e-05, 1.77810271e-05,\n",
      "        2.32044744e-05, 6.68770634e-06],\n",
      "       [2.69231223e-13, 1.56000422e-13, 3.21461618e-13, 1.96714747e-13,\n",
      "        9.35967213e-01, 6.32315447e-14, 3.08093807e-14, 5.46390034e-14,\n",
      "        7.13052538e-14, 2.05409653e-14],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.99999999e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        9.99999998e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
      "        0.00000000e+00, 0.00000000e+00],\n",
      "       [2.21260420e-04, 3.38629235e-04, 1.32602115e-04, 6.73843805e-05,\n",
      "        8.26828681e-01, 1.04738620e-04, 1.59204258e-02, 5.77034306e-05,\n",
      "        8.27141248e-05, 7.32222091e-05],\n",
      "       [2.42705562e-21, 3.67028846e-21, 1.46332111e-21, 7.29225530e-22,\n",
      "        9.62464915e-01, 1.14089499e-21, 1.90807640e-19, 6.22818432e-22,\n",
      "        8.88271346e-22, 8.07246205e-22]]))\n"
     ]
    }
   ],
   "source": [
    "print((topic_model.get_representative_docs()[3]))\n",
    "print(topic_model.transform(topic_model.get_representative_docs()[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Try topic modeling per sentence instead of per review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the reviews, take the average sentiment value of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is only needed once\n",
    "# import nltk\n",
    "\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "reviews = product_df['review_body'].to_list()\n",
    "compounds = list()\n",
    "for review in reviews:\n",
    "    compounds.append(sia.polarity_scores(review)['compound'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the average sentiment value of each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[756.1321090567033, 511.6332144563165, 543.2662668539549, 548.4678688851308, 243.84466015925722, 298.2141129570286, 227.2045497818215, 306.21864006143727, 445.87074947620835, 133.25638921140825]\n",
      "[485.9275532114037, 323.46324071801484, 394.35791634886056, 354.96863762192135, 173.98700611099204, 187.3688032197755, 148.67480777751254, 191.6105828144392, 286.56074147196097, 102.6906799796142]\n",
      "[1.5560593427138851, 1.581735264015185, 1.3775969603545772, 1.5451164152403412, 1.4015107542209255, 1.5915889296001766, 1.5281980395887003, 1.5981301009766646, 1.555938008765361, 1.2976483283377016]\n"
     ]
    }
   ],
   "source": [
    "sents = [0]*nr_topics\n",
    "cum_weights = [0]*nr_topics\n",
    "topic_model_info = topic_model.get_topic_info().drop([0]).reset_index()\n",
    "for index, compound in enumerate(compounds):\n",
    "    for topic, row in topic_model_info.iterrows():\n",
    "        weight = predictions[1][index][topic]\n",
    "        sents[topic] += weight*compound\n",
    "        cum_weights[topic] += weight\n",
    "average_sents = [0]*nr_topics\n",
    "for i in range(nr_topics):\n",
    "    average_sents[i] = sents[i] / cum_weights[i]\n",
    "\n",
    "## average_sents is now a value between 0 and 2 for each topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the $L_2$ norm of each word vector of each review, then calculate the average per review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank all the reviews according to the topic relevance, average sentiment value and information estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the reranked 300 reviews of the most reviewed amazon product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the information value of the summary with the next word predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the relevance of the summary to the product category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the most common English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('researchTopics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b30b14a64463b48a472ddf210762c2b31ca134fd217a9af0c6fba74bd25d541d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
